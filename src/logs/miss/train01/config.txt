IMAGE_SIZE = 100
  51 MAP_GRID_SIZE = 0.05
  52 MAP_RANGE = 5.0
  53 ROBOT_RSIZE = 0.13
  54 SLEEP_TIME = 10
  55 
  56 # for DDQN
  57 ENV = 'FFMP-v0'
  58 GAMMA = 0.90 # discount factor
  59 MAX_STEPS = 100
  60 NUM_EPISODES = 100000
  61 LOG_DIR = "./logs/train01"
  62 # BATCH_SIZE = 1024 # minibatch size
  63 BATCH_SIZE = 64# minibatch size
  64 CAPACITY = 200000 # replay buffer size
  65 # INPUT_CHANNELS = 12 #[channel] = (occupancy(MONO) + flow(RGB)) * series(3 steps)
  66 # INPUT_CHANNELS = 3 #[channel] = (occupancy(MONO) + flow(xy)) * series(1 steps)
  67 INPUT_CHANNELS = 1 # only temporal_bev_image
  68 NUM_ACTIONS = 28
  69 LEARNING_RATE = 0.0005 # learning rate
  70 # LOSS_THRESHOLD = 0.1 # threshold of loss
  71 LOSS_THRESHOLD = 0.000000001 # threshold of loss
  72 LOSS_MEMORY_CAPACITY = 10
  73 REACH_RATE_THRESHOLD = 0.80
  74 REACH_MEMORY_CAPACITY = 10
  75 UPDATE_TARGET_EPISODE = 10
  76 MAX_TOTAL_STEP = 100000
  77 MODEL_PATH = './model/train01/model.pth'

    def reward_calculator(self, relative_goal_info, is_collision, is_goal, is_first):
 131         r_g = 0.0
 132         r_c = 0.0
 133         r_t = 0.0
 134         r_arr = 500.0 / 500.0
 135         r_col = -500.0 / 500.0
 136         r_s = -50.0 / 500.0
 137         epsilon = 100.0 / 500.0
 138
 139         global pre_relative_goal_dist
 140         if is_first:
 141             pre_relative_goal_dist = relative_goal_info[0]
 142
 143         cur_relative_goal_dist = relative_goal_info[0] #[0]:range, [1]:orientation
 144
 145         if is_goal:
 146             r_g = r_arr
 147         else:
 148             r_g = epsilon * (pre_relative_goal_dist - cur_relative_goal_dist)
 149
 150         if is_collision:
 151             r_c = r_col
 152         else:
 153             r_c = 0
 154
 155         r_t = r_g + r_c + r_s
 156
 157         return r_t

