IMAGE_SIZE = 100
  51 MAP_GRID_SIZE = 0.05
  52 MAP_RANGE = 5.0
  53 ROBOT_RSIZE = 0.13
  54 SLEEP_TIME = 10
  55 
  56 # for DDQN
  57 ENV = 'FFMP-v0'
  58 GAMMA = 0.90 # discount factor
  59 MAX_STEPS = 100
  60 NUM_EPISODES = 100000
  61 LOG_DIR = "./logs/experiment3"
  62 # BATCH_SIZE = 1024 # minibatch size
  63 BATCH_SIZE = 64# minibatch size
  64 CAPACITY = 200000 # replay buffer size
  65 # INPUT_CHANNELS = 12 #[channel] = (occupancy(MONO) + flow(RGB)) * series(3 steps)
  66 # INPUT_CHANNELS = 3 #[channel] = (occupancy(MONO) + flow(xy)) * series(1 steps)
  67 INPUT_CHANNELS = 1 # only temporal_bev_image
  68 NUM_ACTIONS = 28
  69 LEARNING_RATE = 0.0001 # learning rate
  70 # LOSS_THRESHOLD = 0.1 # threshold of loss
  71 LOSS_THRESHOLD = 1.0 # threshold of loss
  72 LOSS_MEMORY_CAPACITY = 10
  73 REACH_RATE_THRESHOLD = 0.80
  74 REACH_MEMORY_CAPACITY = 10
  75 UPDATE_TARGET_EPISODE = 5
  76 MODEL_PATH = './model/model3.pth'

 def reward_calculator(self, relative_goal_info, is_collision, is_goal, is_first):
 131         r_g = 0.0
 132         r_c = 0.0
 133         r_t = 0.0
 134         r_arr = 500.0 / 500.0
 135         r_col = -500.0 / 500.0
 136         r_s = -5.0 / 500.0
 137         epsilon = 10.0 / 500.0
