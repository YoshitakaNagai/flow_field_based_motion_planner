# for ROS
IMAGE_SIZE = 100
MAP_GRID_SIZE = 0.05
MAP_RANGE = 5.0
ROBOT_RSIZE = 0.13
SLEEP_TIME = 10

# for DDQN
ENV = 'FFMP-v0'
GAMMA = 0.90 # discount factor
MAX_STEPS = 100
NUM_EPISODES = 100000
LOG_DIR = "./logs/experiment1"
# BATCH_SIZE = 1024 # minibatch size
BATCH_SIZE = 64# minibatch size
CAPACITY = 200000 # replay buffer size
# INPUT_CHANNELS = 12 #[channel] = (occupancy(MONO) + flow(RGB)) * series(3 steps)
# INPUT_CHANNELS = 3 #[channel] = (occupancy(MONO) + flow(xy)) * series(1 steps)
INPUT_CHANNELS = 1 # only temporal_bev_image
NUM_ACTIONS = 28
LEARNING_RATE = 0.0005 # learning rate
# LOSS_THRESHOLD = 0.1 # threshold of loss
LOSS_THRESHOLD = 1.0 # threshold of loss
LOSS_MEMORY_CAPACITY = 10
REACH_RATE_THRESHOLD = 0.80
REACH_MEMORY_CAPACITY = 10
UPDATE_TARGET_EPISODE = 10

def reward_calculator(self, relative_goal_info, is_collision, is_goal, is_first):
 131         r_g = 0.0
 132         r_c = 0.0
 133         r_t = 0.0
 134         r_arr = 1000.0 / 1000.0
 135         r_col = -500.0 / 1000.0
 136         r_s = -5.0 / 1000.0
 137         epsilon = 50.0 / 1000.0
